<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeFTAN-Series</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 5px;
        }
        header {
            text-align: center;
            padding: 5px;
            background-color: #f2f2f2;
        }
        section {
            margin: 5px 0;
        }
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin-bottom: 10px;
        }
    </style>
</head>
<body>

    <header>
        <h1>DeFTAN: Dense Frequency-Time Attentive Network for <br>Multichannel Speech Enhancement and Separation</h1>
    </header>

    <section>
        <h2>What is DeFTAN?</h2>
        <ul>
                &nbsp;&nbsp;With the advancement of deep learning, the field of speech enhancement and separation has experienced significant growth over the past decade. In Smart Sound Systems Lab., we have been researching multi-channel speech enhancement that leverages spatial information to achieve higher performance in recent years.<br>

&nbsp;&nbsp;DeFTAN, an abbreviation for Dense Frequency-Time Attentive Network, is a model where dense blocks utilizing spatial information, F-transformers focusing on frequency information, and T-Conformers utilizing time information are serially connected. Starting from a state-of-the-art model for noisy reverberant speech enhancement tasks, DeFTAN has expanded its scope to include real-time processing, speech separation, real-world experiments, and array-geometry-agnostic speech enhancement.<br>

&nbsp;&nbsp;Currently, our research extends beyond the domain of speech, exploring universal source separation and related areas. The following contents represent subpages for each DeFTAN series. We hope you find what you are looking for, and discussions with peer researchers are always welcome!
        </ul>
        <h2>Contents</h2>
        <ul>
            <li>
                <a href="https://github.com/donghoney0416/DeFT-AN">1) DeFT-AN (2023 IEEE SPL & ICASSP 2023)</a>
                <ul>
                       &nbsp;&nbsp;In this study, we propose a dense frequency-time attentive network (DeFT-AN) for multichannel speech enhancement. DeFT-AN is a mask estimation network that predicts a complex spectral masking pattern for suppressing the noise and reverberation embedded in the short-time Fourier transform (STFT) of an input signal. The proposed mask estimation network incorporates three different types of blocks for aggregating information in the spatial, spectral, and temporal dimensions. It utilizes a spectral transformer with a modified feed-forward network and a temporal conformer with sequential dilated convolutions. The use of dense blocks and transformers dedicated to the three different characteristics of audio signals enables more comprehensive enhancement in noisy and reverberant environments. The remarkable performance of DeFT-AN over state-of-the-art multichannel models is demonstrated based on two popular noisy and reverberant datasets in terms of various metrics for speech quality and intelligibility.
                </ul>
            </li>
            <li>
                <a href="https://github.com/donghoney0416/DeFT-AN-RT">2) DeFT-AN-RT (Interspeech 2023)</a>
                <ul>
                       &nbsp;&nbsp;In real-time speech enhancement models based on the short-time Fourier transform (STFT), algorithmic latency induced by the STFT window size can induce perceptible delays, leading to reduced immersion in real-time applications. This study proposes an efficient real-time enhancement model based on dense frequency-time attentive network (DeFT-AN). The vanilla DeFT-AN consists of cascaded dense blocks and timefrequency transformers, which allow for a smooth transition between time frames through a temporal attention mechanism. To inherit this advantage and reduce algorithmic latency, we develop the lightweight and causal version of DeFT-AN with dualwindow size processing that utilizes synthesis windows shorter than analysis windows. The benefit of DeFT-AN in identifying temporal context enables the use of non-overlapping synthesis windows, and experimental results show that the model can achieve the highest performance with the lowest algorithmic latency among STFT-based models
                </ul>
            </li>
            <li>
                <a href="https://donghoney0416.github.io/demos-DeFTAN-II/demo_page.html">3) DeFTAN-II (Submitted to IEEE TASLP)</a>
                <ul>
                       &nbsp;&nbsp;In this work, we present DeFTAN-II, an efficient multichannel speech enhancement model based on transformer architecture and subgroup processing. Despite the success of transformers in speech enhancement, they face challenges in capturing local relations, reducing the high computational complexity, and lowering memory usage. To address these limitations, we introduce subgroup processing in our model, combining subgroups of locally emphasized features with other subgroups containing original features. The subgroup processing is implemented in several blocks of the proposed network. In the proposed split dense blocks extracting spatial features, a pair of subgroups is sequentially concatenated and processed by convolution layers to effectively reduce the computational complexity and memory usage. For the F- and T-transformers extracting temporal and spectral relations, we introduce crossattention between subgroups to identify relationships between locally emphasized and non-emphasized features. The dualpath feedforward network then aggregates attended features in terms of the gating of local features processed by dilated convolutions. Through extensive comparisons with state-of-the-art multichannel speech enhancement models, we demonstrate that DeFTAN-II with subgroup processing outperforms existing methods at significantly lower computational complexity. Moreover, we evaluate the modelâ€™s generalization capability on realworld data without fine-tuning, which further demonstrates its effectiveness in practical scenarios.
                </ul>
            </li>
            <li>
                4) AGA-DeFTAN (Submitted to Interspeech 2024) - Coming soon!
            </li>
        </ul>
    </section>

    <footer>
        <p>&copy; DeFTAN Series</p>
    </footer>

</body>
</html>